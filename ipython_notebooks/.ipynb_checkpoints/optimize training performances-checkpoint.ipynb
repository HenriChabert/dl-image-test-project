{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataiku.use_plugin_libs('deeplearning-image-v2')\n",
    "from dku_deeplearning_image.misc_objects import DkuModel, DkuImageGenerator\n",
    "import dku_deeplearning_image.dku_constants as constants\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "from dku_config import DSSParameter, DkuConfig\n",
    "from dku_deeplearning_image.recipes import RetrainRecipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dku_deeplearning_image.utils as utils\n",
    "import dku_deeplearning_image.dku_constants as constants\n",
    "import math\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as resnet50_preprocessing\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    Edit: Do not try to externalize, decorator is called before function is defined, there will be an error.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return utils.threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "\n",
    "class DkuImageGenerator(object):\n",
    "    def __init__(self, images_folder, labels, input_shape, batch_size, preprocessing,\n",
    "                 use_augmentation, extra_images_gen=None, n_augm=None):\n",
    "        self.images_folder = images_folder\n",
    "        self.labels = labels\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocessing = preprocessing\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.extra_images_gen = extra_images_gen\n",
    "        self.n_augm = n_augm\n",
    "\n",
    "    def _get_batch_size_adapted(self):\n",
    "        return int(self.batch_size / self.n_augm) if self.use_augmentation else self.batch_size\n",
    "\n",
    "    def _preprocess_img(self, images_folder, img_filename):\n",
    "        image = utils.get_cached_file_from_folder(images_folder, img_filename)\n",
    "        return utils.preprocess_img(image, self.input_shape, self.preprocessing)\n",
    "\n",
    "    def _get_augmented_images(self, image):\n",
    "        augm_image = np.tile(image, (self.n_augm, 1, 1, 1))\n",
    "        return next(self.extra_images_gen.flow(augm_image, batch_size=self.n_augm))\n",
    "\n",
    "    def _process_one_image(self, row):\n",
    "        img_filename = row[constants.FILENAME].decode('utf-8') if isinstance(row[constants.FILENAME], bytes) else row[constants.FILENAME]\n",
    "        label = row[constants.LABEL].decode('utf-8') if isinstance(row[constants.LABEL], bytes) else row[constants.LABEL]\n",
    "        label_index = self.labels.index(label)\n",
    "#         print(\"preprocessing: \", img_filename)\n",
    "        try:\n",
    "            image = self._preprocess_img(self.images_folder, img_filename)\n",
    "            if self.use_augmentation:\n",
    "                X_batch = self._get_augmented_images(image)\n",
    "                y_batch = [label_index] * self.n_augm\n",
    "            else:\n",
    "                X_batch = [image]\n",
    "                y_batch = [label_index]\n",
    "        except IOError as e:\n",
    "            logger.info(\"Cannot read the image '{}', skipping it. Error: {}\".format(img_filename, e))\n",
    "            X_batch, y_batch = [], []\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def _get_batch_features(self, batch_df):\n",
    "        X_batch_list = []\n",
    "        y_batch_list = []\n",
    "\n",
    "        for index, row in batch_df.iterrows():\n",
    "            X_batch, y_batch = self._process_one_image(row)\n",
    "            X_batch_list.extend(X_batch)\n",
    "            y_batch_list.extend(y_batch)\n",
    "\n",
    "        X_batch = np.array(X_batch_list)\n",
    "\n",
    "        actual_batch_size = X_batch.shape[0]\n",
    "        y_batch = np.zeros((actual_batch_size, len(self.labels)))\n",
    "        y_batch[range(actual_batch_size), y_batch_list] = 1\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    @threadsafe_generator\n",
    "    def load(self, image_df):\n",
    "        image_df = pd.DataFrame(image_df, columns=[\"__dku__image_filename\", \"__dku__image_label\"])\n",
    "        n_images = image_df.shape[0]\n",
    "        batch_size_adapted = self._get_batch_size_adapted()\n",
    "        n_batch = int(math.ceil(n_images * 1.0 / batch_size_adapted))\n",
    "        while True:\n",
    "            for num_batch in range(n_batch):\n",
    "                batch_df = image_df.iloc[num_batch * batch_size_adapted: (num_batch + 1) * batch_size_adapted, :]\n",
    "                yield self._get_batch_features(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_label_df(label_dataset, col_filename, col_label):\n",
    "    renaming_mapping = {\n",
    "        col_filename: constants.FILENAME,\n",
    "        col_label: constants.LABEL\n",
    "    }\n",
    "    label_df = label_dataset.get_dataframe().rename(columns=renaming_mapping)[list(renaming_mapping.values())]\n",
    "    return label_df\n",
    "\n",
    "def build_train_test_sets(label_df, train_ratio, random_seed):\n",
    "    train_df, test_df = train_test_split(\n",
    "        label_df,\n",
    "        stratify=label_df[constants.LABEL],\n",
    "        train_size=train_ratio,\n",
    "        random_state=random_seed)\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = dataiku.Folder(\"3ep5yCky\")\n",
    "model_folder = dataiku.Folder(\"VrGKntsf\")\n",
    "labels_dataset = dataiku.Dataset(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_filename = \"path\"\n",
    "col_label = \"label\"\n",
    "\n",
    "label_df = format_label_df(labels_dataset, col_filename, col_label)\n",
    "train_df, test_df = build_train_test_sets(label_df, 0.8, 1887)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import UnidentifiedImageError, ImageFile\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "configs = DkuConfig(**{\n",
    "    'should_use_gpu': {'value': False},\n",
    "    'gpu_usage': {'value': 'all'},\n",
    "    'gpu_list': {'value': []},\n",
    "    'gpu_memory_allocation_mode': {'value': 'memory_limit'},\n",
    "    'gpu_memory_limit': {'value': 31},\n",
    "    'col_filename': {'value': 'path'},\n",
    "    'col_label': {'value': 'label'},\n",
    "    'train_ratio': {'value': 0.9},\n",
    "    'input_shape': {'value': (197, 197, 3)},\n",
    "    'batch_size': {'value': 10},\n",
    "    'model_pooling': {'value': 'max'},\n",
    "    'model_reg': {'value': {'l1': 0.2, 'l2': 0.1}},\n",
    "    'model_dropout': {'value': 0.5},\n",
    "    'layer_to_retrain': {'value': 'n_last'},\n",
    "    'layer_to_retrain_n': {'value': 2},\n",
    "    'optimizer': {'value': 'adam'},\n",
    "    'learning_rate': {'value': 0.001},\n",
    "    'custom_params_opti': {'value': []},\n",
    "    'nb_epochs': {'value': 5},\n",
    "    'nb_steps_per_epoch': {'value': 5},\n",
    "    'nb_validation_steps': {'value': 5},\n",
    "    'data_augmentation': {'value': False},\n",
    "    'n_augmentation': {'value': 0},\n",
    "    'custom_params_data_augment': {'value': []},\n",
    "    'use_tensorboard': {'value': True},\n",
    "    'random_seed': {'value': 1338}\n",
    "})\n",
    "\n",
    "def get_cached_file_from_folder(folder, file_path):\n",
    "    try:\n",
    "        filename = file_path[0].replace(b'/', b'_')\n",
    "    except Exception as err:\n",
    "        raise err\n",
    "    with folder.get_download_stream(file_path) as stream:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(stream.read())\n",
    "            logger.info(f\"cached file {file_path}\")\n",
    "    return filename\n",
    "\n",
    "def preprocess_img(img_path, img_shape, preprocessing):\n",
    "#     print(\"preprocessing: \", img_path)\n",
    "    try:\n",
    "        img = load_img(img_path, target_size=img_shape)\n",
    "    except UnidentifiedImageError as err:\n",
    "        logger.warning(f'The file {img_path} is not a valid image. skipping it. Error: {err}')\n",
    "        return\n",
    "    array = img_to_array(img)\n",
    "    array = preprocessing(array)\n",
    "    return array\n",
    "\n",
    "def convert_target_to_np_array(target_array):\n",
    "    dummies = pd.get_dummies(target_array)\n",
    "    return {\"remapped\": dummies.values.astype(np.int8), \"classes\": list(dummies.columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_old_way():\n",
    "    def build_tfds(pddf):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "             dku_generator.load,\n",
    "             output_types=(tf.float32, tf.float32),\n",
    "             output_shapes=(tf.TensorShape([None, 197, 197, 3]), tf.TensorShape([None, 2])),\n",
    "             args=(pddf,)\n",
    "        )\n",
    "    \n",
    "    retrain_recipe = build_retrain_recipe()\n",
    "\n",
    "    dku_generator = DkuImageGenerator(\n",
    "        images_folder=images_folder,\n",
    "        labels=retrain_recipe.dku_model.get_distinct_labels(),\n",
    "        input_shape=configs.input_shape,\n",
    "        batch_size=configs.batch_size,\n",
    "        preprocessing=retrain_recipe.dku_model.application.preprocessing,\n",
    "        use_augmentation=False\n",
    "    )\n",
    "\n",
    "    train_tfds_old_way, test_tfds_old_way = build_tfds(train_df), build_tfds(test_df)\n",
    "    model_fit(retrain_recipe, train_tfds_old_way, test_tfds_old_way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(images_folder, image_filename, image_shape):\n",
    "    image_path = tf.numpy_function(lambda x: get_cached_file_from_folder(images_folder, x), [image_filename], tf.string)\n",
    "    return tf.numpy_function(lambda x: preprocess_img(x, image_shape, resnet50_preprocessing), [image_path], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from tables import NaturalNameWarning\n",
    "warnings.filterwarnings('ignore', category=NaturalNameWarning)\n",
    "\n",
    "def build_retrain_recipe():\n",
    "    retrain_recipe = RetrainRecipe(configs)\n",
    "\n",
    "    retrain_recipe.load_dku_model(model_folder, label_df)\n",
    "    retrain_recipe.compile()\n",
    "\n",
    "    model_weights_path = retrain_recipe.dku_model.get_weights_path()\n",
    "\n",
    "    callbacks = retrain_recipe._get_callbacks(\n",
    "        output_model_folder=dataiku.Folder('hWWMMUFI'),\n",
    "        model_weights_path=model_weights_path\n",
    "    )\n",
    "    return retrain_recipe\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('{}  {:.2f} ms'.format(method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@timeit\n",
    "def model_fit(retrain_recipe, train_tfds, test_tfds):\n",
    "    model_weights_path = retrain_recipe.dku_model.get_weights_path()\n",
    "\n",
    "    callbacks = retrain_recipe._get_callbacks(\n",
    "        output_model_folder=dataiku.Folder('hWWMMUFI'),\n",
    "        model_weights_path=model_weights_path\n",
    "    )\n",
    "    retrain_recipe._retrain(\n",
    "        train_generator=train_tfds,\n",
    "        test_generator=test_tfds,\n",
    "        callback_list=callbacks\n",
    "    )\n",
    "\n",
    "def retrain(train_tfds, test_tfds, opt_func=lambda x: x):\n",
    "    retrain_recipe = build_retrain_recipe()\n",
    "    model_fit(retrain_recipe, opt_func(train_tfds), opt_func(test_tfds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimized_datasets(train_df, test_df, paral=True):\n",
    "    ### Training Data\n",
    "    # Building train dataset\n",
    "    train_X_tfds = tf.data.Dataset.from_tensor_slices(train_df[\"__dku__image_filename\"].values.reshape(-1, 1))\n",
    "    train_X_tfds = train_X_tfds.map(lambda x: preprocess_image(images_folder, x, configs.input_shape),\n",
    "                                    num_parallel_calls=(-1 if paral else None))\n",
    "\n",
    "    # Building test dataset\n",
    "    test_X_tfds = tf.data.Dataset.from_tensor_slices(test_df[\"__dku__image_filename\"].values.reshape(-1, 1))\n",
    "    test_X_tfds = test_X_tfds.map(lambda x: preprocess_image(images_folder, x, configs.input_shape),\n",
    "                                  num_parallel_calls=(-1 if paral else None))\n",
    "\n",
    "    ### Validation Data\n",
    "    # Building train dataset\n",
    "    train_y_values = convert_target_to_np_array(train_df[\"__dku__image_label\"].values)\n",
    "    train_y_tfds = tf.data.Dataset.from_tensor_slices(train_y_values[\"remapped\"])\n",
    "\n",
    "    # Building test dataset\n",
    "    test_y_values = convert_target_to_np_array(test_df[\"__dku__image_label\"].values)\n",
    "    test_y_tfds = tf.data.Dataset.from_tensor_slices(test_y_values[\"remapped\"])\n",
    "\n",
    "    ### Gathering datasets\n",
    "    train_tfds_common = tf.data.Dataset.zip((train_X_tfds, train_y_tfds)).batch(configs.batch_size, drop_remainder=True).repeat()\n",
    "    test_tfds_common = tf.data.Dataset.zip((test_X_tfds, test_y_tfds)).batch(configs.batch_size, drop_remainder=True).repeat()\n",
    "    \n",
    "    return train_tfds_common, test_tfds_common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hchabert/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 - 8s - accuracy: 0.6200 - loss: 27.5999 - val_accuracy: 0.7500 - val_loss: 23.0304\n",
      "Epoch 2/5\n",
      "5/5 - 5s - accuracy: 0.7143 - loss: 25.7568 - val_accuracy: 0.8611 - val_loss: 21.7466\n",
      "Epoch 3/5\n",
      "5/5 - 5s - accuracy: 0.8333 - loss: 23.2617 - val_accuracy: 1.0000 - val_loss: 20.9861\n",
      "Epoch 4/5\n",
      "5/5 - 5s - accuracy: 0.9286 - loss: 21.3702 - val_accuracy: 0.9167 - val_loss: 20.5420\n",
      "Epoch 5/5\n",
      "5/5 - 5s - accuracy: 0.9762 - loss: 20.4558 - val_accuracy: 1.0000 - val_loss: 19.7459\n",
      "model_fit  35440.46 ms\n",
      "CPU times: user 1min 40s, sys: 15.9 s, total: 1min 56s\n",
      "Wall time: 40.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Old way\n",
    "retrain_old_way()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hchabert/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 - 11s - accuracy: 0.5400 - loss: 28.1289 - val_accuracy: 0.5000 - val_loss: 26.4571\n",
      "Epoch 2/5\n",
      "5/5 - 7s - accuracy: 0.6600 - loss: 25.9002 - val_accuracy: 0.7000 - val_loss: 24.2476\n",
      "Epoch 3/5\n",
      "5/5 - 8s - accuracy: 0.7000 - loss: 22.6347 - val_accuracy: 0.6000 - val_loss: 22.4071\n",
      "Epoch 4/5\n",
      "5/5 - 8s - accuracy: 0.7600 - loss: 23.3522 - val_accuracy: 0.9000 - val_loss: 21.1457\n",
      "Epoch 5/5\n",
      "5/5 - 8s - accuracy: 0.9400 - loss: 20.7426 - val_accuracy: 0.8000 - val_loss: 21.5976\n",
      "model_fit  50039.66 ms\n",
      "CPU times: user 2min 25s, sys: 24.8 s, total: 2min 50s\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# No Optimization\n",
    "train_tfds_common, test_tfds_common = build_optimized_datasets(train_df, test_df, paral=False)\n",
    "retrain(train_tfds_common, test_tfds_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Epoch 1/5\n",
      "5/5 - 7s - accuracy: 0.4800 - loss: 29.7724 - val_accuracy: 0.7000 - val_loss: 24.0917\n",
      "Epoch 2/5\n",
      "5/5 - 6s - accuracy: 0.6600 - loss: 25.8165 - val_accuracy: 0.8000 - val_loss: 22.4083\n",
      "Epoch 3/5\n",
      "5/5 - 6s - accuracy: 0.7600 - loss: 23.0411 - val_accuracy: 0.8000 - val_loss: 21.8577\n",
      "Epoch 4/5\n",
      "5/5 - 7s - accuracy: 0.8200 - loss: 22.2436 - val_accuracy: 1.0000 - val_loss: 20.2780\n",
      "Epoch 5/5\n",
      "5/5 - 6s - accuracy: 0.9000 - loss: 20.7942 - val_accuracy: 1.0000 - val_loss: 19.6407\n",
      "model_fit  41918.84 ms\n",
      "CPU times: user 2min 32s, sys: 24.1 s, total: 2min 56s\n",
      "Wall time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# With parralele preprocessing\n",
    "train_tfds_common, test_tfds_common = build_optimized_datasets(train_df, test_df, paral=True)\n",
    "retrain(train_tfds_common, test_tfds_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Epoch 1/5\n",
      "5/5 - 9s - accuracy: 0.3800 - loss: 32.0228 - val_accuracy: 0.6000 - val_loss: 29.9594\n",
      "Epoch 2/5\n",
      "5/5 - 6s - accuracy: 0.6000 - loss: 29.8514 - val_accuracy: 0.5000 - val_loss: 23.7029\n",
      "Epoch 3/5\n",
      "5/5 - 6s - accuracy: 0.6600 - loss: 25.6862 - val_accuracy: 0.8000 - val_loss: 22.3687\n",
      "Epoch 4/5\n",
      "5/5 - 7s - accuracy: 0.7600 - loss: 23.5567 - val_accuracy: 1.0000 - val_loss: 20.7287\n",
      "Epoch 5/5\n",
      "5/5 - 7s - accuracy: 0.8600 - loss: 21.5015 - val_accuracy: 0.8000 - val_loss: 20.5641\n",
      "model_fit  43904.32 ms\n",
      "CPU times: user 2min 31s, sys: 25 s, total: 2min 56s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# With Cache\n",
    "train_tfds_common, test_tfds_common = build_optimized_datasets(train_df, test_df, paral=True)\n",
    "retrain(train_tfds_common, test_tfds_common, lambda x: x.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": " There appears to be a concurrent caching iterator running - cache lockfile already exists ('./_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1616623558\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]] [Op:__inference_test_function_71363]\n\nFunction call stack:\ntest_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d834e0f7b0d3>\u001b[0m in \u001b[0;36mretrain\u001b[0;34m(train_tfds, test_tfds, opt_func)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tfds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tfds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mretrain_recipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_retrain_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrain_recipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tfds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tfds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0f888c0d58b8>\u001b[0m in \u001b[0;36mtimed\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtimed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'log_time'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d834e0f7b0d3>\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(retrain_recipe, train_tfds, test_tfds)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tfds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtest_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_tfds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcallback_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/plugins/dev/deeplearning-image-v2/python-lib/dku_deeplearning_image/recipes/retrain_recipe.py\u001b[0m in \u001b[0;36m_retrain\u001b[0;34m(self, train_generator, test_generator, callback_list)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/plugins/dev/deeplearning-image-v2/python-lib/dku_deeplearning_image/misc_objects/dku_model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetattrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    870\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m    873\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    648\u001b[0m               *args, **kwds)\n\u001b[1;32m    649\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.dss/dss-8/dss-home/code-envs/python/deeplearning-image-for-ntb/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m:  There appears to be a concurrent caching iterator running - cache lockfile already exists ('./_0.lockfile'). If you are sure no other running TF computations are using this cache prefix, delete the lockfile and re-initialize the iterator. Lockfile contents: Created at: 1616623558\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n\t [[RemoteCall]]\n\t [[IteratorGetNextAsOptional]] [Op:__inference_test_function_71363]\n\nFunction call stack:\ntest_function\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# With prefetch\n",
    "train_tfds_common, test_tfds_common = build_optimized_datasets(train_df, test_df, paral=True)\n",
    "retrain(train_tfds_common, test_tfds_common, lambda x: x.cache('./').prefetch(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache():\n",
    "    for f in [x for x in os.listdir('./') if x.endswith('.jpg') or x.endswith('.jpeg')]:\n",
    "        os.remove(f)\n",
    "    print(\"The cache has been cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cache has been cleared\n"
     ]
    }
   ],
   "source": [
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_weights_notop.h5', 'remote-run-env-def.json', 'model_weights.h5']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "creator": "admin",
  "customFields": {},
  "kernelspec": {
   "display_name": "Python (env deeplearning-image-for-ntb)",
   "language": "python",
   "name": "py-dku-venv-deeplearning-image-for-ntb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
